#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{qtree}
\usepackage{tree-dvips}
\usepackage{ulem}
\usepackage{graphicx}
\newsavebox{\partbox}
\DeclareMathOperator*{\argmax}{arg\,max}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A Topical Collocation Model
\end_layout

\begin_layout Author
Benjamin Börschinger
\end_layout

\begin_layout Date
09/12/2013
\end_layout

\begin_layout Standard
These are some notes on a non-grammar version of Mark Johnson's Topic Collocatio
n Model.
 While equivalent to the original model, this re-formulation allows for
 faster inference using a break-point sampler, rather than having to perform
 actual parsing.
 In addition, the sampler we describe might be applied to related uses of
 the Collocation-modeling idea, e.g.
 in word-segmentation.
\end_layout

\begin_layout Section
The Topic Collocation Grammar
\end_layout

\begin_layout Standard
Johnson (2010) introduced the Topic Collocation Grammar (TCG) that is a
 non-parametric extension of LDA.
 Whereas the HDP (Teh et al., 2006) uses non-parametric priors to allow for
 an unbounded number of possible topics, the TCG uses non-parametric priors
 to allow for an unbounded vocabulary, making it possible to learn semantically
 coherent topical units such as 
\begin_inset Quotes eld
\end_inset

electrical circuit board
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
It's original presentation makes use of Adaptor Grammars, an extension of
 PCFGs which allows for a simple specification of certain models by defining
 the generative process as that of generating a tree using context-free
 rules.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Base} & =\text{CollocGen}(\Phi_{Base},p_{\#})\\
\theta_{i} & \sim\text{Dir}(\boldsymbol{\alpha}_{\theta})\\
\phi_{k} & \sim\text{DP}(\alpha_{\phi},\text{Base})\\
z_{i,1}\mid\boldsymbol{\theta} & \sim\text{Disc}(\theta_{i})\\
P(z_{i,j}=\text{STOP}\mid\boldsymbol{\theta},p_{Stop}) & =p_{stop}\\
P(z_{i,j}=x\mid\boldsymbol{\theta},p_{Stop}) & =(1-p_{stop})\theta_{i,x}\\
c_{i,j}\mid z_{i,j},\boldsymbol{\phi} & \sim\phi_{z_{i,j}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In words, we assume a base-distribution which can generate sequences of
 words.
 We describe this distribution in more detail below although we simply assume
 a Unigram process with fixed emission probabilities 
\begin_inset Formula $\Phi_{Base}$
\end_inset

 and a fixed termination-probability 
\begin_inset Formula $p_{\#}$
\end_inset

.
 Fixing these parameters simplifies implementation of the inference algorithm
 but is not required.
 It is easy to put a Dirichlet prior on 
\begin_inset Formula $\Phi_{Base}$
\end_inset

 and a Beta on 
\begin_inset Formula $p_{\#}$
\end_inset

.
\end_layout

\begin_layout Standard
For each of the 
\begin_inset Formula $N$
\end_inset

 documents, we draw a document-specific mixture over topics, one 
\begin_inset Formula $\theta_{i}$
\end_inset

 for each document 
\begin_inset Formula $1\leq i\leq N$
\end_inset

.
 We also generate 
\begin_inset Formula $K$
\end_inset

 topics which are distributions over collocations, that is, sequences of
 words.
 This is achieved by drawing each 
\begin_inset Formula $\phi_{k}$
\end_inset

 from a DP which takes as input the Base distribution.
\end_layout

\begin_layout Standard
Finally, we generate the documents by definitely generating a first topic-indica
tor.
 We keep generating topic-indicators until we generate the stop-symbol,
 the math might have to be written differently, but it should convey the
 idea.
 Finally, the actual collocations are generated from the respective topic-specif
ic distributions.
\end_layout

\begin_layout Standard
Linking it back to Johnson's original formulation, our Base-distribution
 corresponds to the distribution defined by the 
\begin_inset Quotes eld
\end_inset

Words
\begin_inset Quotes erd
\end_inset

-non-terminal, the 
\begin_inset Formula $\phi_{i}$
\end_inset

-distributions correspond to the 
\begin_inset Quotes eld
\end_inset


\bar under
Topic
\begin_inset Formula $_{i}$
\end_inset


\bar default

\begin_inset Quotes erd
\end_inset

 adapted non-terminals, and the 
\begin_inset Formula $\theta$
\end_inset

s, 
\begin_inset Formula $z$
\end_inset

s and 
\begin_inset Formula $p_{stop}$
\end_inset

 parameters capture the document-mixture branching process.
\end_layout

\begin_layout Section
Inference
\end_layout

\begin_layout Standard
Inference in this model is complicated by the fact that we do not actually
 know the number of units in a given document as the actual observations
 are not 
\begin_inset Formula $c_{i,j}$
\end_inset

s but the 
\begin_inset Formula $w_{i,k}$
\end_inset

s that make up the collocations.
 This is illustrated by a toy example using 3 words which, already, are
 compatible with 4 different hypotheses about the extent of the 
\begin_inset Formula $c_{i,j}$
\end_inset

s, illustrated by bracketing.
\end_layout

\begin_layout Itemize
(electric circuit boards)
\begin_inset Formula $_{1}$
\end_inset


\end_layout

\begin_layout Itemize
(electric)
\begin_inset Formula $_{1}$
\end_inset

(circuit)
\begin_inset Formula $_{2}$
\end_inset

(boards)
\begin_inset Formula $_{3}$
\end_inset


\end_layout

\begin_layout Itemize
(electric circuit)
\begin_inset Formula $_{1}$
\end_inset

(boards)
\begin_inset Formula $_{2}$
\end_inset


\end_layout

\begin_layout Itemize
(electric)
\begin_inset Formula $_{1}$
\end_inset

(circuit boards)
\begin_inset Formula $_{2}$
\end_inset


\end_layout

\begin_layout Standard
In addition to the actual grouping, the assignment to topics is also latent,
 as in standard topic-models.
 Assuming a single topic, however, we can immediately make the connection
 to the Goldwater Unigram model of Word Segmentation model, equating words
 in the TCG case with characters in the WS case.
 Goldwater introduced a conceptually simple and easy to implement sampler
 for her Word Segmentation by realizing that each possible hypothesis about
 the latent word-variables corresponds to a binary vector of boundary indicators
, and that it is easy to derive a Gibbs-Sampler over this representation.
\end_layout

\begin_layout Standard
We will apply this idea directly to the TCG in the following paragraph.
 We will focus on the case of a single document for simplicity.
\end_layout

\begin_layout Subsection
State-space
\end_layout

\begin_layout Standard
The following latent-variables need to be performed inference for.
\end_layout

\begin_layout Itemize
the document specific distribution over topics 
\begin_inset Formula $\theta$
\end_inset


\end_layout

\begin_layout Itemize
the 
\begin_inset Formula $K$
\end_inset

 topic distributions 
\begin_inset Formula $\phi_{k}$
\end_inset

, all of which are draws from (independent) DPs
\end_layout

\begin_layout Itemize
the 
\begin_inset Formula $N$
\end_inset

 topic indicators 
\begin_inset Formula $z_{i}$
\end_inset

, where 
\begin_inset Formula $N$
\end_inset

 is the (latent) number of collocations
\end_layout

\begin_layout Itemize
the actual 
\begin_inset Formula $N$
\end_inset

 collocations 
\begin_inset Formula $c_{i}$
\end_inset

, where we constrain these such that their linear concatenation gives rise
 to the actually observed sequence of words 
\begin_inset Formula $w_{1:N'}$
\end_inset

 of 
\begin_inset Formula $N'$
\end_inset

 words
\end_layout

\begin_layout Standard
We integrate out 
\begin_inset Formula $\theta$
\end_inset

 and, crucially, all of the 
\begin_inset Formula $K$
\end_inset

 topic distributions, giving rise to the well-known Posterior Predictive
 Distributions described by the CRP.
 If the base-distribution for the DPs is fixed, we can ignore the actual
 seating arrangements as we have a non-hierarchical DP model in this case,
 simplifying book-keeping.
 It's straight-forward to derive the added complications a non-fixed base-distri
bution raises (see Börschinger and Johnson (2011) for details).
\end_layout

\begin_layout Standard
The actual state-space of our sampler consists of 
\begin_inset Formula $N'$
\end_inset

 
\begin_inset Formula $K$
\end_inset

-valued variables that indicate the presence of a Collocation boundary,
 as well as the Topic from which this collocation came.
 This is analogous to the Goldwater (2007) sampler, except that rather than
 a binary choice a 
\begin_inset Formula $K$
\end_inset

+1-ary choice needs to be made at every possible break-point.
\end_layout

\begin_layout Subsubsection
Example
\end_layout

\begin_layout Standard
Assume 
\begin_inset Formula $w_{1:4}=\text{red\,\ electric\,\ circuit\,\ boards}$
\end_inset

, and our current indicator vector is 
\begin_inset Formula $b=\langle3,0,0,1\rangle$
\end_inset

.
 This vector encodes the following analysis
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
(\text{red})_{3}(\text{electric circuit boards})_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
That is, there are two collocations, one spanning the single word 
\begin_inset Quotes eld
\end_inset

red
\begin_inset Quotes erd
\end_inset

 and coming from Topic 3, and one spanning the words 
\begin_inset Quotes eld
\end_inset

electric circuit boards
\begin_inset Quotes erd
\end_inset

 and coming from Topic 1.
 It is trivial to map this back to latent variables and then, calculate
 this state's probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
z_{1} & =3\\
z_{2} & =1\\
z_{3} & =\text{STOP}\\
c_{1} & =\text{red}\\
c_{2} & =\text{electric circuit boards}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(z_{1},z_{2},z_{3},c_{1},c_{2}\mid\mu)=\frac{\alpha_{\theta}}{K\alpha_{\theta}}\text{CollocGen}(\text{red})(1-p_{stop})\frac{\alpha_{\theta}}{K\alpha_{\theta}+1}\text{CollocGen}(\text{electric circuit boards})p_{stop}
\]

\end_inset


\end_layout

\begin_layout Standard
Note how the topic-distribution has been 
\begin_inset Quotes eld
\end_inset

updated
\begin_inset Quotes erd
\end_inset

, hence the denominator has an additional count because 
\begin_inset Formula $z_{1}$
\end_inset

 had to be generated.
 Because this is all the data there is, the topics have to back-off to the
 base, hence we only have WordGen-factors.
 For illustration, if 
\begin_inset Formula $z_{1}=1$
\end_inset

, we would have gotten
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula 
\[
P(z_{1},z_{2},z_{3},c_{1},c_{2}\mid\mu)=\frac{\alpha_{\theta}}{K\alpha_{\theta}}\text{CollocGen}(\text{red})(1-p_{stop})\frac{\alpha_{\theta}+1}{K\alpha_{\theta}+1}\frac{\alpha_{\phi}\text{CollocGen}(\text{electric circuit boards})}{\alpha_{\phi}+1}p_{stop}
\]

\end_inset


\end_layout

\begin_layout Standard
Let us now resample the value for the first variable.
 Removing knowledge about it gives the partial vector 
\begin_inset Formula $b=\langle?,0,0,1\rangle$
\end_inset

 which, just as in word segmentation, requires us to be non-commital with
 all affected aspects of the previous analysis.
 In fact, this affects the entire document as the next boundary already
 is the end of the document.
 Consequently, we would need to
\end_layout

\begin_layout Itemize
remove one count for having used Topic 3
\end_layout

\begin_layout Itemize
remove onecount for having used Topic 1
\end_layout

\begin_layout Itemize
remove one count for having 
\begin_inset Quotes eld
\end_inset

red
\begin_inset Quotes erd
\end_inset

 in Topic 3
\end_layout

\begin_layout Itemize
remove one count for having 
\begin_inset Quotes eld
\end_inset

electric circuit boards
\begin_inset Quotes erd
\end_inset

 in Topic 1
\end_layout

\begin_layout Itemize
adjust the overall count of generated Topics / collocations (if there were
 
\begin_inset Formula $n_{3}$
\end_inset

 uses of Topic 3 before, now there are 
\begin_inset Formula $n_{3}-1$
\end_inset

; if there were 
\begin_inset Formula $n_{3,red}$
\end_inset

 uses of 
\begin_inset Quotes eld
\end_inset

red
\begin_inset Quotes erd
\end_inset

 in Topic 3, now there are 
\begin_inset Formula $n_{3,red}-1$
\end_inset

 such uses
\end_layout

\begin_layout Standard
Having made these adjustments, we can calculate the probability of all possible
 ways of setting 
\begin_inset Formula $?$
\end_inset

.
 Setting it to 
\begin_inset Formula $0$
\end_inset

 would give rise to the analysis
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
(\text{red electric circuit boards})_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
via the new set of latent-variables 
\begin_inset Formula $z_{1}=1$
\end_inset

, 
\begin_inset Formula $z_{2}=\text{STOP}$
\end_inset

, 
\begin_inset Formula $c_{1}=$
\end_inset

red electric circuit boards.
 We have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(z_{1},z_{2},c_{1}\mid\mu)=\frac{\alpha_{\theta}}{K\alpha_{\theta}}\text{CollocGen}(\text{red electric circuit boards})p_{stop}
\]

\end_inset


\end_layout

\begin_layout Standard
Let's consider setting 
\begin_inset Formula $?$
\end_inset

 to any specific topic 
\begin_inset Formula $x\in\{1..K\}$
\end_inset

.
 This gives rise to 
\begin_inset Formula $z_{1}=x$
\end_inset

, 
\begin_inset Formula $z_{2}$
\end_inset

=1 (note how this value doesn't change because you condition on 
\begin_inset Formula $b_{4}=1$
\end_inset

), 
\begin_inset Formula $z_{3}=$
\end_inset

STOP, 
\begin_inset Formula $c_{1}=$
\end_inset

red, 
\begin_inset Formula $c_{2}$
\end_inset

=electric circuit boards.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
(\text{red})_{x}(\text{electric circuit boards})_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
So,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(z_{1},z_{2},z_{3},c_{1},c_{2}\mid\mu)=\frac{\alpha_{\theta}}{K\alpha_{\theta}}\text{CollocGen}(\text{red})(1-p_{stop})\frac{\alpha_{\theta}+\mathbb{I}_{x=1}}{K\alpha_{\theta}+\mathbb{I}_{x=1}}\frac{\alpha_{\phi}\text{CollocGen}(\dots)+\mathbb{I}_{x=1}}{\alpha_{\phi}+\mathbb{I}_{x=1}}p_{stop}
\]

\end_inset


\end_layout

\begin_layout Standard
That's basically it.
 Note that you also need to resample the final value of 
\begin_inset Formula $b$
\end_inset

 which must not take 0, as there always is a known utterance boundary but
 you still need to resample the topic of the final collocation.
 The most important thing is efficient book-keeping, as usually, there's
 lots of utterances and types.
 And efficient identification of the parts of the current analysis that
 change, such that you can update the counts.
\end_layout

\end_body
\end_document
